---
title: "Breast Cancer Identification"
author: "Dimensionless"
date: "October 29, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
### Objective 
In an almost universal paradigm, the CAD problem is addressed by a 4 stage system:

1)Candidate generation which identifies suspicious unhealthy candidate regions of interest (candidate ROIs, or simply candidates) from a medical image;
2) Feature extraction which computes descriptive features for each candidate so that each candidate is represented by a vector x of numerical values or attributes;
classification which differentiates candidates that are malignant cancers from the rest of the candidates based on x; and
visual presentation of CAD findings to the radiologist.
In this challenge, we focus on stage 3, learning the classifier to differentiate malignant cancers from other candidates.
### Importing the data from "Breast Cancer Info"
```{r Importing}
info<-read.table("Breast Cancer Info.tsv",header = F,sep = "\t")
info$V12<-NULL
col_names<-c("label","image-finding-id","study-finding-id","image-id","patient-id","leftbreast","MLO","x-location","y-location","x-nipple-location","y-nipple-location")
colnames(info)<-col_names
# Importing features file
features<-read.delim("Features.txt",header = F)
features$V118<-NULL
```
### Exploring the data
```{r Exploration}
str(info)
table(info$label)
table(info$`image-finding-id`)
table((subset(info,info$`patient-id`==14280))$`image-id`)
library(dplyr)
info%>%filter(`patient-id`==14280 )%>%select(`study-finding-id`)
length(unique(info$`patient-id`))
unique(info$`study-finding-id`)
```
### Combining the data
```{r}
dataset<-cbind(info,features)
class(dataset)
```
### Splitting the data
```{r Splitting}
library(caTools)
set.seed(1)
split<-sample.split(dataset$label,SplitRatio = 0.5)
table(split)
train<-dataset[split,]
table(train$label)
length(unique(train$`patient-id`))
test<-dataset[!split,]
length(unique(test$`patient-id`))
```
### Making the model
```{r Model}
# Starting by logistic regression
model_data<-train[,c(1,12:128)]
model1<-glm(as.factor(label)~.,data = model_data,family = "binomial")
summary(model1)
table(train$label,model1$fitted.values>0.2)
#Making Predictions
pred_test<-predict(model1,newdata = test,type = "response")
#Calculating AUC
library(ROCR)
ROCRpred<-prediction(pred_test,as.factor(test$label))
ROCRperf<-performance(ROCRpred,"tpr","fpr")
ROCRperf<-performance(ROCRpred,"auc",fpr.stop=0.2)
ROCRperf<-performance(ROCRpred,"auc",fpr.stop=0.3)

ROCRperf
plot(ROCRperf,colorize=TRUE)
abline(v = c(0.2,0.3))
ROCRperf@y.values
# AUC in the 0.2-0.3 FP range = 0.0914576
# Confusion matrix
table(test$label,pred_test>=0.2)
# Cross validating
library(boot)
cv.error<-cv.glm(train,glmfit = model1,K = 10)
```
### Applying PCA
```{r PCA}
pca<-prcomp(model_data[,-1],scale=T)
pca$
pc_var<-pca$sdev^2
pve<-100*pc_var/sum(pc_var)
library(ggplot2)
qplot(y=cumsum(pve),x=1:length(pve),geom = c("point","line"))
# Only taking first 60 component to build the model.
predictor_space<-cbind(label=model_data$label,pca$x[,1:60])
predictor_space<-as.data.frame(predictor_space)
dim(predictor_space)
# Applying logistic regression 
model_pca<-glm(as.factor(label)~.,data = predictor_space,family = "binomial")
table(predictor_space$label,model_pca$fitted.values>0.2)
# Making predictions
test_pca<-predict(pca,test)
test_pca<-as.data.frame(test_pca)
pred_test_pca<-predict(model_pca,test_pca,type = "response")
#Apply ROC and calculate AUC 
ROCRpred<-prediction(pred_test_pca,as.factor(test$label))
ROCRperf<-performance(ROCRpred,"tpr","fpr")
ROCRperf_0.2<-performance(ROCRpred,"auc",fpr.stop=0.2)
ROCRperf_0.3<-performance(ROCRpred,"auc",fpr.stop=0.3)
ROCRperf_0.2@y.values
ROCRperf_0.3@y.values
ROCRperf_0.3@y.values[[1]]-ROCRperf_0.2@y.values[[1]]
plot(ROCRperf,colorize=TRUE)
abline(v = c(0.2,0.3))
```
### Applyin knn 
```{r}
library(class)
model_knn<-knn(train = predictor_space[,-1],test=test_pca[,1:60],cl = predictor_space$label,k = 1)

table(test$label,model_knn)
```
### Applying QDA
```{r}
library(MASS)
model_qda<-qda(label~.,data=model_data)
model_qda$prior
model_qda$scaling
# Making predictions 

pred_test<-predict(model_qda,newdata = test)
ROCRpred<-prediction(pred_test$posterior[,2],as.factor(test$label))
ROCRperf<-performance(ROCRpred,"tpr","fpr")
plot(ROCRperf,colorize=TRUE)
abline(v = c(0.2,0.3))
ROCRperf_0.2<-performance(ROCRpred,"auc",fpr.stop=0.2)
ROCRperf_0.3<-performance(ROCRpred,"auc",fpr.stop=0.3)
AUC<-(ROCRperf_0.3@y.values[[1]]-ROCRperf_0.2@y.values[[1]])
hist(model_data$V14)
summary(model_data$V5)
```
